{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import plotly.graph_objects as go\n",
    "import scipy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mean1, std1, prior1, mean2, std2, prior2 = sp.symbols('x mean1 std1 prior1 mean2 std2 prior2')\n",
    "\n",
    "# Define the likelihood function\n",
    "def normal_likelihood(x, mean, std, prior):\n",
    "    return (1/((2*sp.pi)**(1/2) * std)) * sp.exp((-1/2)*((x-mean)/std)**2) * prior\n",
    "\n",
    "# Define the posterior probability function\n",
    "def normal_posterior_probability(x, mean1, std1, prior1, mean2, std2, prior2):\n",
    "    likelihood1 = normal_likelihood(x, mean1, std1, prior1)\n",
    "    likelihood2 = normal_likelihood(x, mean2, std2, prior2)\n",
    "    return likelihood1 / (likelihood1 + likelihood2)\n",
    "\n",
    "# Define the classifier\n",
    "def normal_classifier(x, mean1, std1, prior1, mean2, std2, prior2):\n",
    "    px = normal_likelihood(x, mean1, std1, prior1) + normal_likelihood(x, mean2, std2, prior2)\n",
    "    return -((x - mean1) ** 2 ) / (2*(std1**2)) - (sp.log(2*sp.pi)/2) - sp.log(std1) + sp.log(prior1) - sp.log(px)\n",
    "\n",
    "# Vectorize the functions if needed\n",
    "normal_l = np.vectorize(normal_likelihood)\n",
    "normal_pp = np.vectorize(normal_posterior_probability)\n",
    "normal_c = np.vectorize(normal_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for plotting\n",
    "x_values = np.linspace(-8,8)  # Adjust the range as needed\n",
    "\n",
    "mean_1, std_1, prior_1  = -1, 0.4, 0.5\n",
    "mean_2, std_2, prior_2 = 1, 0.6, 0.5\n",
    "\n",
    "# Plot the graph\n",
    "#plt.figure(figsize=(10, 6))\n",
    "plt.ylim(0,1)\n",
    "#plt.plot(x_values, y_values, label='likelyhood', color='blue')\n",
    "plt.plot(x_values, normal_l(x_values, mean_1, std_1, prior_1), color = \"yellow\")\n",
    "plt.plot(x_values, normal_l(x_values, mean_2, std_2, prior_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_values, normal_pp(x_values, mean_1, std_1, prior_1, mean_2, std_2, prior_2), label='post', color='red')\n",
    "plt.plot(x_values, normal_pp(x_values, mean_2, std_2, prior_2, mean_1, std_1, prior_1), label='post2', color='green')\n",
    "for result in sp.solve(sp.Eq(normal_classifier(x, mean_1, std_1, prior_1, mean_2, std_2, prior_2) - normal_classifier(x, mean_2, std_2, prior_2, mean_1, std_1, prior_1), 0), x):\n",
    "    plt.axvline(x = result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, mean1, covariance1, prior1, mean2, covariance2, prior2 = sp.symbols('x1 x2 mean1 covariance1 prior1 mean2 covariance2 prior2')\n",
    "\n",
    "def multi_attr_likelyhood(x1, x2, mean, covariance, prior):\n",
    "    x = np.array([x1, x2])\n",
    "    front = (1/((2 * np.pi) ** (covariance.shape[0]/2) * np.linalg.det(covariance) ** (1/2)))\n",
    "    back = np.exp(-(1/2)*np.dot(np.dot(np.transpose(x - mean), np.linalg.inv(covariance)), (x - mean)))\n",
    "    return front * back * prior\n",
    "\n",
    "def multi_attr_posterier_prob(x1, x2, mean1, covariance1, prior1, mean2, covariance2, prior2):\n",
    "    likelihood1 = multi_attr_likelyhood(x1, x2, mean1, covariance1, prior1)\n",
    "    likelihood2 = multi_attr_likelyhood(x1, x2, mean2, covariance2, prior2)\n",
    "    return likelihood1 / (likelihood1 + likelihood2)\n",
    "\n",
    "def multi_attr_likelyhood_sym(x1, x2, mean1, covariance1, prior1):\n",
    "    x = sp.Matrix([x1, x2])\n",
    "    front = (1/((2 * sp.pi) ** (covariance1.shape[0]/2) * (covariance1.det()) ** (1/2)))\n",
    "    back = sp.exp(-1/2) * (x - mean1).T * covariance1 ** (-1) * (x - mean1)\n",
    "    return front * back * prior1\n",
    "\n",
    "def multi_attr_classifier(x1, x2, mean1, covariance1, prior1, mean2, covariance2, prior2):\n",
    "    x = sp.Matrix([x1, x2])\n",
    "    px = multi_attr_likelyhood_sym(x1, x2, mean1, covariance1, prior1) + multi_attr_likelyhood_sym(x1, x2, mean2, covariance2, prior2)\n",
    "    return (-(1/2) * (x - mean1).T * covariance1 ** (-1) * (x - mean1))[0] - (covariance1.shape[0]/2) * sp.log(2 * sp.pi) - (1/2) * sp.log(covariance1.det()) + sp.log(prior1) - sp.log(px[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for plotting\n",
    "x1_values = np.linspace(-8,8,100)  # Adjust the range as needed\n",
    "x2_values = np.linspace(-8,8,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1_values, x2_values)\n",
    "\n",
    "mean_1, var_1, prior_1  = np.array([-1,-1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "mean_2, var_2, prior_2  = np.array([1,1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "\n",
    "Z1 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z1[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_1, var_1, prior_1)\n",
    "\n",
    "Z2 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z2[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_2, var_2, prior_2)\n",
    "\n",
    "#fig = go.Figure(data=go.Contour(x=x1_values, y=x2_values, z=Z1, contours_coloring='lines',line_width=2))\n",
    "\n",
    "#fig.update_traces(ncontours=20, selector=dict(type='contour'))\n",
    "\n",
    "#Customize the contour plot if needed\n",
    "#fig.update_layout(\n",
    "    #title='Contour Plot',\n",
    "    #xaxis_title='X1',\n",
    "    #yaxis_title='X2',\n",
    "#)\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "plt.contour(X1, X2, Z1, colors = \"red\", linewidths=1)\n",
    "plt.contour(X1, X2, Z2, colors = \"blue\", linewidths=1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Likelihoods')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for plotting\n",
    "x1_values = np.linspace(-8,8,100)  # Adjust the range as needed\n",
    "x2_values = np.linspace(-8,8,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1_values, x2_values)\n",
    "\n",
    "mean_1, var_1, prior_1  = np.array([-1,-1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "mean_2, var_2, prior_2  = np.array([1,1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "\n",
    "Z1 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z1[i,j] = multi_attr_posterier_prob(x1_values[i], x2_values[j], mean_1, var_1, prior_1, mean_2, var_2, prior_2)\n",
    "\n",
    "Z2 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z2[i,j] = multi_attr_posterier_prob(x1_values[i], x2_values[j], mean_2, var_2, prior_2, mean_1, var_1, prior_1)\n",
    "\n",
    "fig = go.Figure(data=go.Contour(x=x1_values, y=x2_values, z=Z1, contours_coloring='lines', colorscale='tealrose',line_width=2))\n",
    "\n",
    "fig.update_traces(ncontours=20, selector=dict(type='contour'))\n",
    "\n",
    "#Customize the contour plot if needed\n",
    "fig.update_layout(\n",
    "    title='Contour Plot',\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=go.Contour(x=x1_values, y=x2_values, z=Z2, contours_coloring='lines', colorscale='tealrose',line_width=2))\n",
    "\n",
    "fig.update_traces(ncontours=20, selector=dict(type='contour'))\n",
    "\n",
    "#Customize the contour plot if needed\n",
    "fig.update_layout(\n",
    "    title='Contour Plot',\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig = go.Figure(data=go.Surface(x=x1_values, y=x2_values, z=Z2, colorscale='tealrose'))\n",
    "\n",
    "fig.add_trace(go.Surface(x=x1_values, y=x2_values, z=Z1, colorscale='RdBu'))\n",
    "\n",
    "fig.update_traces(ncontours=20, selector=dict(type='contour'))\n",
    "\n",
    "#Customize the contour plot if needed\n",
    "fig.update_layout(\n",
    "    title='Contour Plot',\n",
    "    xaxis_title='X1',\n",
    "    yaxis_title='X2',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for plotting\n",
    "x1_values = np.linspace(-8,8,100)  # Adjust the range as needed\n",
    "x2_values = np.linspace(-8,8,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1_values, x2_values, indexing=\"ij\")\n",
    "\n",
    "mean_1, var_1, prior_1  = np.array([-1,-1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "mean_2, var_2, prior_2  = np.array([1,1]), np.array([[1, 0], [0, 1]]), 0.5\n",
    "\n",
    "Z1 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z1[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_1, var_1, prior_1)\n",
    "\n",
    "Z2 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z2[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_2, var_2, prior_2)\n",
    "\n",
    "plt.contour(X1, X2, Z1, colors = \"red\", linewidths=1)\n",
    "plt.contour(X1, X2, Z2, colors = \"blue\", linewidths=1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Likelihoods and Decision Boundaries')\n",
    "\n",
    "mean_1, var_1, prior_1  = sp.Matrix([-1, -1]), sp.Matrix([[1, 0], [0, 1]]), 0.5\n",
    "mean_2, var_2, prior_2  = sp.Matrix([1, 1]), sp.Matrix([[1, 0], [0, 1]]), 0.5\n",
    "\n",
    "for sol in sp.solve(sp.Eq(multi_attr_classifier(x1, x2, mean_1, var_1, prior_1, mean_2, var_2, prior_2), \n",
    "                     multi_attr_classifier(x1, x2, mean_2, var_2, prior_2, mean_1, var_1, prior_1)), x1,  dict = True) :\n",
    "    func = sp.lambdify((x2), sol[x1], 'numpy')\n",
    "    vec_func = np.vectorize(func)\n",
    "    plt.plot(vec_func(x2_values), x2_values)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values for plotting\n",
    "x1_values = np.linspace(-8,8,100)  # Adjust the range as needed\n",
    "x2_values = np.linspace(-8,8,100)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1_values, x2_values, indexing=\"ij\")\n",
    "\n",
    "mean_1, var_1, prior_1  = np.array([2,0]), np.array([[0.5, 0], [0, 0.25]]), 0.5\n",
    "mean_2, var_2, prior_2  = np.array([1,0]), np.array([[0.25, 0], [0, 0.5]]), 0.5\n",
    "\n",
    "Z1 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z1[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_1, var_1, prior_1)\n",
    "\n",
    "Z2 = np.zeros(X1.shape)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X2.shape[1]):\n",
    "      Z2[i,j] = multi_attr_likelyhood(x1_values[i], x2_values[j], mean_2, var_2, prior_2)\n",
    "\n",
    "plt.contour(X1, X2, Z1, colors = \"red\", linewidths=1)\n",
    "plt.contour(X1, X2, Z2, colors = \"blue\", linewidths=1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Likelihoods and Decision Boundaries')\n",
    "\n",
    "mean_1, var_1, prior_1  = sp.Matrix([2, 0]), sp.Matrix([[0.5, 0], [0, 0.25]]), 0.5\n",
    "mean_2, var_2, prior_2  = sp.Matrix([1, 0]), sp.Matrix([[0.25, 0], [0, 0.5]]), 0.5\n",
    "\n",
    "for sol in sp.solve(sp.Eq(multi_attr_classifier(x1, x2, mean_1, var_1, prior_1, mean_2, var_2, prior_2), \n",
    "                     multi_attr_classifier(x1, x2, mean_2, var_2, prior_2, mean_1, var_1, prior_1)), x1,  dict = True) :\n",
    "    func = sp.lambdify((x2), sol[x1], 'numpy')\n",
    "    vec_func = np.vectorize(func)\n",
    "    plt.plot(vec_func(x2_values), x2_values, color = \"green\")\n",
    "\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "dataset1 = make_moons(noise=0.3, random_state=0)\n",
    "dataset2 = make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "dataset3 = linearly_separable\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "\n",
    "plt.scatter(dataset1[0][:,0], dataset1[0][:,1], c=dataset1[1], cmap = cm_bright)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import plotly.graph_objs as go\n",
    "# Manually define your training data\n",
    "X = dataset3[0]\n",
    "y = dataset3[1]  # Binary class labels (0 or 1)\n",
    "\n",
    "# Define a function to add polynomial features with a specified degree\n",
    "def add_polynomial_features(X, degree):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    # Select the columns for X, Y, and XY if they exist\n",
    "    #if degree >= 2:\n",
    "        #X_poly = X_poly[:, [1, 2, 4]]\n",
    "    #else:\n",
    "        #X_poly = X_poly[:, [1, 2]]\n",
    "    \n",
    "    # Add a column of ones to the most left column\n",
    "    ones_column = np.ones((X_poly.shape[0], 1))\n",
    "    X_poly_selected = np.concatenate((ones_column, X_poly), axis=1)\n",
    "    \n",
    "    return X_poly_selected\n",
    "\n",
    "\n",
    "# Specify the polynomial degree\n",
    "degree = 1  # You can change the degree as needed\n",
    "\n",
    "# Add polynomial features\n",
    "X_poly_selected = add_polynomial_features(X, degree)\n",
    "# Calculate the number of features generated by the polynomial transformation\n",
    "num_features = X_poly_selected.shape[1]\n",
    "\n",
    "# Initialize different initial theta values with the correct number of elements\n",
    "initial_theta1 = np.zeros(num_features)  # Initial theta 1\n",
    "\n",
    "# Define the learning rate and number of iterations\n",
    "alpha = 1\n",
    "iterations = 2000\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the cost function\n",
    "def compute_cost(theta, X, y):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    # print(h)\n",
    "    J = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return J\n",
    "\n",
    "# Gradient Descent to update theta\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    cost_history = []\n",
    "    cost = compute_cost(theta, X, y)\n",
    "    cost_history.append(cost)\n",
    "    for _ in range(iterations):\n",
    "        h = sigmoid(np.dot(X, theta))\n",
    "        gradient = (1/m) * np.dot((h - y),X)\n",
    "        theta -= alpha * gradient\n",
    "        cost = compute_cost(theta, X, y)\n",
    "        cost_history.append(cost)\n",
    "        # print(\"theta1\",theta)\n",
    "        # print(\"cost:\",cost)\n",
    "    return theta, cost_history\n",
    "\n",
    "print(\"theta1\",initial_theta1)\n",
    "# Perform gradient descent with initial theta 1\n",
    "theta1, cost_history1 = gradient_descent(X_poly_selected, y, initial_theta1, alpha, iterations)\n",
    "print(\"theta1\",theta1)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the decision boundaries for both initial thetas\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "grid_poly = add_polynomial_features(np.c_[xx.ravel(), yy.ravel()], degree)\n",
    "\n",
    "# Check if there are only 3 columns in grid_poly\n",
    "# if grid_poly.shape[1] == 4:\n",
    "grid_poly_selected = grid_poly\n",
    "# else:\n",
    "#     grid_poly_selected = grid_poly[:, [1, 2, 4]]\n",
    "\n",
    "Z1 = sigmoid(np.dot(grid_poly_selected, theta1))\n",
    "Z1 = Z1.reshape(xx.shape)\n",
    "\n",
    "# Create a Plotly 3D surface plot\n",
    "trace1 = go.Surface(x=xx, y=yy, z=Z1, colorscale='RdBu', opacity=0.8)\n",
    "trace2 = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=y, mode='markers', marker=dict(size=4, color=y, colorscale='RdBu'))\n",
    "\n",
    "layout = go.Layout(scene=dict(xaxis_title='Feature 1', yaxis_title='Feature 2', zaxis_title='Probability'))\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "\n",
    "# Show the interactive plot in Jupyter Notebook or as a separate HTML file\n",
    "fig.show()\n",
    "\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.contourf(xx, yy, Z1, cmap=plt.cm.RdBu, alpha=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the cost history for both initial thetas\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(iterations+1), cost_history1)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs. Iterations (Theta 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
